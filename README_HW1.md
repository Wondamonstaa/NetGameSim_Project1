# CS 441 Project 1 Fall 2023
## Kiryl Baravikou
### UIN: 656339218
### NetID: kbara5

Repo for the NetGameSim Project 1 for CS 441 Fall 2023

---

AWS EMR Deployment YouTube link: EDIT

---

## How to Run the Project:
1) Start by downloading the project repository from Git.
2) Navigate to the NetGameSim directory: use your terminal or command prompt to navigate to the "NetGameSim" directory within the downloaded project folder.
3) In your terminal, run the following command to clean, compile, and execute the project: `sbt clean compile run`.
4) To run tests and ensure the code's functionality, use the following command: `sbt clean test`.
5) If you need to create a .jar file for your project, execute the following command: `sbt clean assembly`. The resulting jar will be placed in __NetGameSim/target/scala-3.2.2/netmodelsim.jar__
6) If you prefer using IntelliJ for development:
    \na. Import the project into IntelliJ IDE.
    \nb. Build the project.
    \nc. Create a "configuration" for the Main.scala file.
    \nd. Set the configuration's arguments to be the input and output folders, separated by a space, for example: <input folder> <output folder>. NB: to run the project properly, a user, i.e. you, should specify the input/output paths in the application.conf file.
7) Ensure that your local input/output folder has the necessary permissions to allow the program to read from and write to it. You can check and adjust folder permissions by right-clicking on the project directory, selecting "Properties," and navigating to the "Security" tab. Make sure that the group "Users" has both "Read" and "Write" permissions.
8) Before running the program, ensure that Hadoop is running on your machine. The program may depend on Hadoop, so it's essential to have it set up and running correctly.
---

## Requirements:

The goal of the project is to create a program for parallel distributed processing of large graphs to produce matching statistics about generated graphs and their perturbed counterparts. The following core functionalities must be met:

1. Statistics Generation: compute a YAML or CSV file that displays the distribution of nodes and edges based on their similarities, calculated using a custom formula.
2. Likelihood Calculation: determine the likelihood that a node or edge was modified, added, or removed in the perturbed graph, or if it remained unchanged. 
3. Custom Graph Matching Algorithm: develop a uniquely fine-tuned graph matching algorithm to make determinations about modifications.
4. Comparison with Golden Set YAML: compare each detected modification with the golden set of changes generated by NetGraphSim when creating a perturbed graph.
5. Algorithm Evaluation: assess the goodness of your algorithm, similar to precision and recall ratios, based on experimentation and results.

Other Requirements:
1) The output files should be in the format of .csv or any other human-readible format.
2) 5 or more scalatests should be implemented.
3) Logging used for all programs.
4) Configurable input and output paths for the map/reduce programs.
5) Compileable through 'sbt clean compile run'
7) Deployed on AWS EMR.
8) Video explanation of the deployment recorded.

---

## Technical Design

We will take a look at the detailed description of how each of these pieces of code work below. Line by line comments explaining every step are also added to the source code in this git repo.:

1) ### RunJobs.scala
	This is the main method that we call to run all our programs. It has been added to simplify the usage of the map/reduce programs, so that we don't have to call the 4 codes individually. It takes our input and output path, and uses them to call each of the 4 files one by one. It passes the input and output paths to the 'main' classes of the files in their arguments.

2) ### DistributionCSV.scala




## Limitations:
1) If the user is running the program locally, they must have Java 8 (or above), sbt 1.6 and Hadoop 3.3.6 installed.
2) The program can handle multiple files in the same input folder if the user wishes to split the file, but it cannot handle input files at different locations. 
3) The user must have the ability to grant Read/Write permissions to the group Users for the LogFileGenerator project folder. This usually requires Administrator access.
4) The functionality for changing the name and extension of the output file works only when running locally (i.e., it does not change the name and extension in S3 when running the program on AWS EMR).
5) 5

